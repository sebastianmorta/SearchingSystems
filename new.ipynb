{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load dataset\n",
    "#df = pd.read_csv(\"datasets/20_newsgroup.csv\") # src: https://www.kaggle.com/c/learn-ai-bbc\n",
    "#df = pd.read_csv(\"datasets/news_headlines.csv\") # src: https://www.kaggle.com/datasets/rmisra/news-category-dataset\n",
    "# TODO: find sport article dataset or sth similar\n",
    "df = pd.read_csv('datasets/nasz.csv', delimiter=';')\n",
    "df = df.astype(str)\n",
    "\n",
    "#df = df.groupby('category', group_keys=False).apply(lambda x: x.sample(1000)) # stratify -- take x elements of each category\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(df.category) # generate class plot\n",
    "print(df['category'].value_counts()) # print count for each class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count number of words in each article\n",
    "\n",
    "df['word_count'] = df['text'].str.len()\n",
    "sns.distplot(df['word_count']).set_title('Article length distribution')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate word cloud for each class\n",
    "\n",
    "def create_wordcloud(words, title):\n",
    "    wordcloud = WordCloud(width=500, height=500).generate(words)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "for category in pd.unique(df['category']):\n",
    "    category_df = df.loc[df['category'] == category]\n",
    "    txt = ' '.join(category_df.text)\n",
    "    create_wordcloud(txt, category)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower().replace('\\n', ' ').replace('\\r', '').strip() # remove special characters\n",
    "    text = re.sub(' +', ' ', text) # remove multiple whitespaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove non-letter characters\n",
    "    # removing stopwords could be added here\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['word_count'] = df['text'].str.len()\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = list(df['text'])\n",
    "y = list(df['category'])\n",
    "print(y[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# label encoding\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "print(y[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size= .8, random_state = 1410)\n",
    "print(f'train: {len(X_train)}, test: {len(X_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "# TF = count of specific word in article / number of words in article\n",
    "# IDF = log(number of articles containing specific word / number of articles)\n",
    "# TFIDF = TF * IDF -- for each article\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=1, norm='l2', encoding='utf-8', ngram_range=(1, 2), stop_words='english')\n",
    "X_train = tfidf.fit_transform(X_train).toarray()\n",
    "X_test = tfidf.transform(X_test).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bag-of-Words\n",
    "count_vectorizer = CountVectorizer(analyzer='word', stop_words='english')\n",
    "X_train = count_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test = count_vectorizer.transform(X_test).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(multi_class=\"multinomial\")\n",
    "rf_model = RandomForestClassifier()\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "models = [lr_model, rf_model, knn_model]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(type(model).__name__)\n",
    "    print(classification_report(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_model=MLPClassifier()\n",
    "\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "print(type(mlp_model).__name__)\n",
    "print(classification_report(y_test,y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(mlp_model, open('mlp_model.pkl', 'wb'))\n",
    "\n",
    "    # load generated traffic from file\n",
    "    # vectors = pickle.load(open('vectors.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def twenty_newsgroup_to_csv():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "    df.columns = ['text', 'target']\n",
    "\n",
    "    targets = pd.DataFrame( newsgroups_train.target_names)\n",
    "    targets.columns=['title']\n",
    "\n",
    "    out = pd.merge(df, targets, left_on='target', right_index=True)\n",
    "    out['date'] = pd.to_datetime('now')\n",
    "    out.to_csv('20_newsgroup.csv')\n",
    "twenty_newsgroup_to_csv()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def upload_data():\n",
    "    df = pd.read_csv('20_newsgroup.csv')\n",
    "    out_df = pd.DataFrame(columns=[\"text\", \"category\"])\n",
    "    out_df['text'] = df['text']\n",
    "    out_df['category'] = df['title']\n",
    "    out_df.to_csv('datasets/20_newsgroup.csv', index=False)\n",
    "upload_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "saved_model = pickle.load(open('mlp_model.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=1,norm='l2', encoding='utf-8', ngram_range=(1, 2), stop_words='english')\n",
    "X = tfidf.fit_transform(X).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = saved_model.predict(X)\n",
    "\n",
    "print(type(mlp_model).__name__)\n",
    "print(label_encoder.inverse_transform(y),label_encoder.inverse_transform(y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}